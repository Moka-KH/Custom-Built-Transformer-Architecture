# Custom-Built-Transformer-Architecture

This project is part of the Neural Networks course at the Faculty of Computer and Information Sciences, Ain Shams University (ASU). Inspired by BERT, this implementation is a fully custom-built transformer model, developed entirely from scratch without relying on built-in deep learning functions.

The project aims to provide a deeper understanding of transformer architectures by constructing each component manually, including self-attention mechanisms, positional encoding, multi-head attention, and feed-forward layers. By building the model from the ground up, we gain valuable insights into the inner workings of modern NLP architectures while reinforcing core concepts in deep learning and sequence modeling.

This repository serves as an educational resource for those looking to explore transformer models at a fundamental level and experiment with their own optimizations.

### The Architecture:
![image](https://github.com/user-attachments/assets/12509a85-517b-4129-ba50-f7d736ec85df)

### Trails:
![image](https://github.com/user-attachments/assets/e0925ab3-eec9-4a64-9c5b-353f2f8366b8)

### Conclusion
The custom-built transformer model with a learning rate of 1e-3, batch size of 64, 20 heads, and 200-dimensional embedding achieves the highest validation accuracy of 69.47%, indicating an effective balance between complexity and performance.

### Contributers
- Malk Khalid https://https/www.linkedin.com/in/malk-khatab
- Mayar Ehab 


=> Acknowledgment: The dataset used in this project was provided by the Neural Network Course teaching team - Faculty of Computer and Information Sciences at Ain Shams University.
