{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PJgi4Fu3bw3",
        "outputId": "b0334ff7-e710-4cfa-9fd8-715d7d21f36c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: tensorflow 2.17.1\n",
            "Uninstalling tensorflow-2.17.1:\n",
            "  Successfully uninstalled tensorflow-2.17.1\n",
            "\u001b[33mWARNING: Skipping keras-nlp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==2.17\n",
            "  Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting keras-nlp==0.5.0\n",
            "  Downloading keras_nlp-0.5.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.17) (1.26.4)\n",
            "Collecting tensorflow-text (from keras-nlp==0.5.0)\n",
            "  Downloading tensorflow_text-2.18.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.17) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow==2.17) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow==2.17) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow==2.17) (3.1.3)\n",
            "INFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tensorflow_text-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow==2.17) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow==2.17) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow==2.17) (0.1.2)\n",
            "Downloading tensorflow-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (601.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m601.3/601.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras_nlp-0.5.0-py3-none-any.whl (527 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.1/527.1 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow, tensorflow-text, keras-nlp\n",
            "Successfully installed keras-nlp-0.5.0 tensorflow-2.17.0 tensorflow-text-2.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y tensorflow keras-nlp\n",
        "!pip install tensorflow==2.17 keras-nlp==0.5.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI28G9B73Dct",
        "outputId": "0d07013e-6b6b-4f49-df4d-92f9a3b53433"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import *\n",
        "import keras_nlp\n",
        "import re\n",
        "import os\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers import Input, Embedding, Dropout, LayerNormalization, Dense, GlobalAveragePooling1D, BatchNormalization, MultiHeadAttention, Bidirectional, LSTM\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import requests\n",
        "import zipfile\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize the lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Constants\n",
        "MAX_WORDS = 10000\n",
        "MAX_LEN = 512\n",
        "EMBEDDING_DIM = 300"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbdLw0sa_bPL"
      },
      "source": [
        "# Create Nessacry Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ZVHlinRoy7eZ"
      },
      "outputs": [],
      "source": [
        "# Download GloVe and create embedding matrix\n",
        "def download_glove_embeddings():\n",
        "    if not os.path.exists('glove.6B.100d.txt'):\n",
        "        print(\"Downloading GloVe embeddings...\")\n",
        "        url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "        response = requests.get(url)\n",
        "        with open('glove.6B.zip', 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall()\n",
        "        os.remove('glove.6B.zip')\n",
        "        print(\"Download complete!\")\n",
        "\n",
        "def create_embedding_matrix(word_index):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    embeddings_index = {}\n",
        "    with open(f'glove.6B.{EMBEDDING_DIM}d.txt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "\n",
        "    print(\"Creating embedding matrix...\")\n",
        "    vocab_size = min(MAX_WORDS, len(word_index) + 1)\n",
        "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "    for word, i in word_index.items():\n",
        "        if i >= MAX_WORDS:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "\n",
        "    return embedding_matrix, vocab_size\n",
        "\n",
        "\n",
        "download_glove_embeddings()\n",
        "\n",
        "#Data preprocessing\n",
        "def preprocessing(text):\n",
        "    #remove URL\n",
        "    #text = re.sub(r'http\\S+|www\\S+', '', str(text))\n",
        "    #remove arabic words\n",
        "    text = ' '.join([word for word in text.split() if not re.match(r'[\\u0600-\\u06FF]', word)])\n",
        "    #remove special characters & puncituations\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s]', '',str(text))\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    #remove stop words\n",
        "    words = text.split()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join([word for word in words if word.lower() not in stop_words])\n",
        "    #Get the word's lemma\n",
        "    text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "#Get the Positional Encoding Vector\n",
        "def get_positional_encoding(max_len, d_model):\n",
        "    positions = np.arange(max_len)[:, np.newaxis]\n",
        "    dimensions = np.arange(d_model)[np.newaxis, :]\n",
        "    angle_rates = 1 / np.power(10000, (2 * (dimensions // 2)) / d_model)\n",
        "    angle_rads = positions * angle_rates\n",
        "\n",
        "    pos_encoding = np.zeros(angle_rads.shape)\n",
        "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "    pos_encoding = tf.cast(pos_encoding, dtype=tf.float32)\n",
        "    return tf.expand_dims(pos_encoding, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcXkUZNtA8ol"
      },
      "source": [
        "# Apply Text Preprocessing and Embidding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyPZyGCRA4oE",
        "outputId": "5fafd6e2-58ed-451a-c1c1-213f9a3f95e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading and preprocessing data...\n",
            "Tokenizing text...\n",
            "Loading GloVe embeddings...\n",
            "Creating embedding matrix...\n",
            "Creating datasets...\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess data\n",
        "print(\"Loading and preprocessing data...\")\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "data = data.drop('SampleID', axis=1)\n",
        "data = data.dropna(subset=['Discussion'])\n",
        "#data['Discussion'] = data.apply(lambda row: f\"This is a {row['Category']} text.\" if pd.isnull(row['Discussion']) else row['Discussion'], axis=1)\n",
        "data['Discussion'] = data['Discussion'].apply(preprocessing)\n",
        "\n",
        "# Tokenization\n",
        "print(\"Tokenizing text...\")\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
        "tokenizer.fit_on_texts(data['Discussion'])\n",
        "sequences = tokenizer.texts_to_sequences(data['Discussion'])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_LEN, padding='post')\n",
        "\n",
        "# Category mapping\n",
        "category_map = {\"Politics\": 0, \"Sports\": 1, \"Media\": 2, \"Market & Economy\": 3, \"STEM\": 4}\n",
        "labels = data['Category'].map(category_map).values\n",
        "\n",
        "# Download GloVe and create embedding matrix\n",
        "download_glove_embeddings()\n",
        "embedding_matrix, vocab_size = create_embedding_matrix(tokenizer.word_index)\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating datasets...\")\n",
        "dataset = tf.data.Dataset.from_tensor_slices((padded_sequences, labels))\n",
        "dataset = dataset.shuffle(buffer_size=2048)\n",
        "\n",
        "\n",
        "#######################################################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTh08jgR_9yk"
      },
      "source": [
        "# Build the Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmN6-3w57r8l"
      },
      "outputs": [],
      "source": [
        "#Build the transformer model\n",
        "def transformer_model(vocab_size, embedding_matrix):\n",
        "    inputs = Input(shape=(MAX_LEN,))\n",
        "\n",
        "    # Embedding layer with pretrained weights\n",
        "    x = Embedding(\n",
        "        vocab_size,\n",
        "        EMBEDDING_DIM,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=False\n",
        "    )(inputs)\n",
        "    x = Dropout(0.2)(x)\n",
        "\n",
        "    # Add positional encoding\n",
        "    pos_encoding = get_positional_encoding(MAX_LEN, EMBEDDING_DIM)\n",
        "    x = x + pos_encoding[:, :MAX_LEN, :]\n",
        "\n",
        "    # First transformer block\n",
        "    attention = MultiHeadAttention(\n",
        "        num_heads=20,\n",
        "        key_dim=200,\n",
        "        dropout=0.5\n",
        "    )(x, x, x)\n",
        "    attention = Dropout(0.1)(attention)\n",
        "    x = LayerNormalization(epsilon=1e-6)(attention + x)\n",
        "\n",
        "    # Feed Forward\n",
        "    \n",
        "    # Global pooling and classification\n",
        "\n",
        "    return Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0lW4_LrBNgx"
      },
      "source": [
        "# Training and Validatinig the Model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
